#!/usr/bin/python3.6

import argparse
import sys
import os


#################################################################
# Import utils directory
from os import path
import pathlib
main_dir = pathlib.Path(__file__).parent.parent.parent.absolute()
sys.path.append(str(main_dir))
#################################################################

import torchvision.transforms as transforms
from torchvision.utils import save_image
from torch.utils.data import DataLoader
from torch.autograd import Variable
from PIL import Image
import torch
import tqdm
import shutil
import statistics
import matplotlib.pylab as plot

from RGB2TIR.utils.models import Generator
from RGB2TIR.utils.models import Discriminator
from RGB2TIR.utils.datasets import get_list_of_files
from RGB2TIR.utils.datasets import ImageDataset
from RGB2TIR.utils.vgg_loss import PerceptualLoss

parser = argparse.ArgumentParser()
parser.add_argument('--batchSize',           type=int, default=1,    help='size of the batches')
parser.add_argument('--input_nc',            type=int, default=1,    help='number of channels of input data')
parser.add_argument('--output_nc',           type=int, default=3,    help='number of channels of output data')
parser.add_argument('--size',                type=int, default=256,  help='size of the data (squared assumed)')
parser.add_argument('--TIR_w',               type=int, default=640,  help='size of the data TIR width (squared assumed)')
parser.add_argument('--TIR_h',               type=int, default=480,  help='size of the data crop (squared assumed)')
parser.add_argument('--RGB_w',               type=int, default=1280, help='size of the data TIR width (squared assumed)')
parser.add_argument('--RGB_h',               type=int, default=960,  help='size of the data crop (squared assumed)')
parser.add_argument('--cuda',                action='store_true',    help='use GPU computation')
parser.add_argument('--n_cpu',               type=int, default=8,    help='number of cpu threads to use during batch generation')
parser.add_argument('--generator_TIR_2_RGB', type=str, default='RGB2TIR/output/p_netG_TIR_2_RGB.pth', help='TIR_2_RGB generator checkpoint file')
parser.add_argument('--generator_RGB_2_TIR', type=str, default='RGB2TIR/output/p_netG_RGB_2_TIR.pth', help='RGB_2_TIR generator checkpoint file')
parser.add_argument('--discriminator_RGB', type=str, default='RGB2TIR/output/p_netD_RGB.pth', help='RGB discriminator checkpoint file')
opt = parser.parse_args()
print(opt)

if torch.cuda.is_available() and not opt.cuda:
    print("WARNING: You have a CUDA device, so you should probably run with --cuda")



###### Definition of variables ######
# Criterions
vgg19_criterion = PerceptualLoss(features_to_compute=['conv5_4'], criterion=torch.nn.L1Loss(), shave_edge=None)
criterion_MSE   = torch.nn.MSELoss()
criterion_L1    = torch.nn.L1Loss()

# Networks
netG_TIR_2_RGB = Generator(opt.input_nc, opt.output_nc ,input_type="TIR")
netG_RGB_2_TIR = Generator(opt.output_nc, opt.input_nc, input_type="RGB")
netD_RGB = Discriminator(opt.output_nc, input_type="RGB")

if opt.cuda:
    netG_TIR_2_RGB.cuda()
    netG_RGB_2_TIR.cuda()
    netD_RGB.cuda()
    vgg19_criterion.cuda()

# Load state dicts
netG_TIR_2_RGB.load_state_dict(torch.load(os.path.join(main_dir, opt.generator_TIR_2_RGB)))
netG_RGB_2_TIR.load_state_dict(torch.load(os.path.join(main_dir, opt.generator_RGB_2_TIR)))
netD_RGB.load_state_dict(torch.load(os.path.join(main_dir, opt.discriminator_RGB)))


# Set model's test mode
netG_TIR_2_RGB.eval()
netG_RGB_2_TIR.eval()

# Inputs & targets memory allocation
Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor
input_A = Tensor(opt.batchSize, opt.input_nc, opt.TIR_w, opt.TIR_h)
input_B = Tensor(opt.batchSize, opt.output_nc, opt.RGB_w, opt.RGB_h)

# Dataset loader
# transforms_ = [transforms.ToTensor(),
#                 transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]
# dataloader = DataLoader(ImageDataset(transforms_=transforms_, mode='test'),
#                         batch_size=opt.batchSize, shuffle=False, num_workers=0)

TIR_transforms_ = [transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (0.5,))]
RGB_transforms_ = [transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
dataloader = DataLoader(ImageDataset(TIR_transforms_=TIR_transforms_, RGB_transforms_=TIR_transforms_, mode='test',
                        unaligned=True), batch_size=opt.batchSize, shuffle=True, num_workers=0)



target_real = Variable(Tensor(opt.batchSize).fill_(1.0), requires_grad=False)
###################################

###### Testing######

# Create output dirs if they don't exist
if os.path.exists(os.path.join(main_dir, 'RGB2TIR/output/RGBtoTIR')):
    shutil.rmtree(os.path.join(main_dir, 'RGB2TIR/output/RGBtoTIR'))
os.makedirs(os.path.join(main_dir, 'RGB2TIR/output/RGBtoTIR'))
if os.path.exists(os.path.join(main_dir, 'RGB2TIR/output/TIRtoRGB')):
    shutil.rmtree(os.path.join(main_dir, 'RGB2TIR/output/TIRtoRGB'))
os.makedirs(os.path.join(main_dir, 'RGB2TIR/output/TIRtoRGB'))

# final_list = get_list_of_files(mode='test')

Gan_loss        = []
Generate_loss   = []
Vgg_loss        = []
print(f"Starting test. {len(dataloader)} files to test")
for i, batch in enumerate(tqdm.tqdm(dataloader, total=len(dataloader), leave=False)):
    if i%10 != 0:
        continue
    # Set model input
    item_RGB = batch['RGB'][:, :, 113:1393, 33:993]

    real_TIR = Variable(input_A.copy_(batch['TIR'][0, :, :, ]))
    real_RGB = Variable(input_B.copy_(item_RGB[0, :, :, :]))


    # Generate output
    # fake_RGB = netG_TIR_2_RGB(real_TIR)
    fake_RGB = netG_TIR_2_RGB(real_TIR)
    fake_TIR = netG_RGB_2_TIR(real_RGB)


    # Check empirical results
    pred_fake = netD_RGB(fake_RGB)
    loss_GAN_TIR_2_RGB = criterion_MSE(pred_fake[0, :], target_real) * 1.4
    Gan_loss.append(loss_GAN_TIR_2_RGB.item())

    loss_RGB_Generate = criterion_L1(fake_RGB, real_RGB)
    Generate_loss.append(loss_RGB_Generate.item())

    loss_vgg_RGB = vgg19_criterion(fake_RGB, real_RGB)
    Vgg_loss.append(loss_vgg_RGB.item())


    fake_RGB = 0.5 * (fake_RGB + 1.0)
    fake_TIR = 0.5 * (fake_TIR + 1.0)

    # Save image files
    save_image(fake_TIR[0], os.path.join(main_dir, 'RGB2TIR/output/RGBtoTIR/%04d.png') % (i+1))
    save_image(fake_RGB[0], os.path.join(main_dir, 'RGB2TIR/output/TIRtoRGB/%04d.png') % (i+1))

    # sys.stdout.write('\rGenerated images %04d of %04d' % (i+1, len(final_list)))

sys.stdout.write('\n')

print('#########################################')
print('summary - generate_loss*2   GAN_loss*1.5')
print('=======')

print(f"vgg loss            - {statistics.mean(Vgg_loss)}")
print(f"L1 RGB compare loss - {statistics.mean(Generate_loss)}")
print(f"discriminator loss  - {statistics.mean(Generate_loss)}")
###################################
